# ğŸš€ All-About-LLMs

This repository is my exploration space for working with **Large Language Models (LLMs)**.  
It serves as a collection of experiments, implementations, and training runs while I learn and build intuition around different aspects of LLMs.  
The goal is not just to replicate existing methods, but also to understand how things work under the hood and gain hands-on experience with model training, fine-tuning, and evaluation.  

---

## ğŸ“‚ Projects

- [x] Supervised Fine-Tuning (SFT) â€“ Pythia-410M on Alpaca  (./SFT)
- [x] NanoGPT (character-level GPT from scratch) (./NanoGPT)
- [ ] BERT from Scratch  
- [ ] LoRA / PEFT Fine-tuning    

---

## âš¡ Tech Stack
- PyTorch  
- Hugging Face Transformers  
- TRL  
- Matplotlib  

---

## ğŸ› ï¸ Work in Progress
This repo will be continuously updated as I explore more methods for training and fine-tuning LLMs.

---

## ğŸ™Œ Acknowledgements
Big thanks to:
- **Andrej Karpathy** for NanoGPT  
- **Hugging Face** for open-source tools  
- **Stanford Alpaca** team for dataset release  
- **EleutherAI** for open LLMs 

---

## ğŸ“œ License
This repository is for **educational and research purposes only**.
